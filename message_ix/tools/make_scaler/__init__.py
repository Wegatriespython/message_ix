import os
import re

import ixmp
import numpy as np
import pandas as pd

from message_ix.tools.lp_diag import LPdiag

lp = LPdiag()
mp = ixmp.Platform()


def filter_df(data, bounds):
    """Extracts matrix elements with coefficient outliers.

    This function extracts elements from a matrix where
    the coefficients deviate from a specified threshold.

    Parameters:
    -----------
    data : pandas DataFrame
        The coefficient matrix, typically generated by
        the mps_sum function in LPDiag. It should have a 'val' column
        containing log10(abs(coefficient)).
    bounds: int or list of 2 integers
        Exponent threshold used to identify outlier coefficients.
        If a single integer is provided, the bounds are set to +/- that value.
        If a list of 2 integers is provided, they represent the lower and
        upper bounds of the threshold.

    Examples:
    ---------
    # Extract elements with coefficient exponents deviating beyond +/-3
    extracted_data = filter_df(data_matrix, 3)

    # Extract elements with coefficients exponents deviating
    # beyond the range of -2 to 2
    extracted_data = extract_outliers(data_matrix, [-2, 2])
    """

    if isinstance(bounds, int):
        lo_bound = -bounds
        up_bound = bounds
    else:
        lo_bound = bounds[0]
        up_bound = bounds[1]

    # Assumes 'data' has a 'val' column with log10(abs(coefficients))
    df_filtered = data.loc[(data["val"] <= lo_bound) | (data["val"] >= up_bound)]

    return df_filtered


def make_logdf_coo(data_coo):
    """
    Make log10 of the absolute non zero value element of a COO DataFrame.
    Input DataFrame has columns like 'row', 'col', 'val'.
    Output DataFrame will have 'val' replaced by log10(abs('val')).
    """
    log_absdf = data_coo.copy()
    non_zero_mask = log_absdf["val"] != 0
    log_absdf.loc[non_zero_mask, "val"] = np.log10(
        np.absolute(log_absdf.loc[non_zero_mask, "val"])
    )
    # For zero values, log10 is undefined. We might fill with a very small number or handle later.
    # For now, they might become NaN or -inf. Let's fill NaNs that resulted from log10(0) or log10(negative from abs error)
    # Or more simply, values that were zero initially should remain non-problematic for log scaling.
    # The filter_df will operate on these log values. Coefficients that were zero won't be outliers.
    log_absdf.loc[
        ~non_zero_mask, "val"
    ] = -np.inf  # Represent log(0) as -infinity for min/max calculations
    return log_absdf


def show_range_coo(data_coo, pretext):
    """
    To display coefficient exponents range from a COO DataFrame.
    Assumes 'val' column contains actual coefficient values (not logs).
    """
    # Create a temporary Series for log calculation, excluding zeros
    vals_for_log = data_coo.loc[data_coo["val"] != 0, "val"]
    if vals_for_log.empty:
        print(f"{pretext}: [N/A | N/A] (no non-zero coefficients)")
        return

    log_abs_vals = np.log10(np.absolute(vals_for_log))

    print(
        f"{pretext}:",
        "[",
        np.int32(np.min(log_abs_vals))
        if not log_abs_vals.empty
        else "N/A",  # lower bound
        "|",
        np.int32(np.max(log_abs_vals))
        if not log_abs_vals.empty
        else "N/A",  # upper bound
        "]",
    )


def get_scaler_args(scenario_ref=None, model="", scenario=""):
    """
    Function to make gams argument for scaling

    """
    if not scenario_ref:
        strings = ["MsgScaler", model, scenario]
    else:
        strings = ["MsgScaler", scenario_ref.model, scenario_ref.scenario]

    file_name = "_".join(s.replace(" ", "_") for s in strings)

    current_directory = os.getcwd()
    two_levels_up = os.path.abspath(os.path.join(current_directory, "../.."))

    prescale_args_dir = os.path.join(two_levels_up, f"model/scaler/{file_name}.gms")

    if os.path.exists(prescale_args_dir):
        return f"--scaler={file_name}"
    else:
        print("The referred scenario doesn't have prescaler file!")
        print("Please use make_prescaler() function to create one")


def make_scaler(path, scen, bounds=4, steps=1, display_range=True):
    """
    Process to generate prescale_args in GAMS to improve
    matrix coefficients.

    This function shifts matrix coefficient exponents to improve
    the scaling properties of the matrix. The function returns
    prescale arguments (prescale_args) to be passed to the GAMS model.

    Parameters:
    -----------
    path: str
        Pathways to locate the mps file.
    scen: ixmp.Scenario
        The scenario object for naming the output GAMS file.
    bounds: int or list of 2 integers
        Exponent threshold used to identify outlier coefficients (log10 scale).
        If a single integer is provided, the bounds are set to +/- that value.
        If a list of 2 integers is provided, they represent the lower and
        upper bounds of the threshold.
    steps: int
        Number of times the prescaler generation process is repeated.
        Larger values may lead to more refined prescale_args but
        also increase computation time.
    display_range: boolean
        Option to show the coefficient exponents range before and after scaling.
        If True, the function will display the range; otherwise, it will not.

    Returns:
    --------
    prescale_args_df: pandas.DataFrame
        A DataFrame of prescale arguments.
    """

    # lp_diag instance
    current_lp = LPdiag()
    current_lp.read_mps(path)

    # Work with the COO matrix from lp_diag
    # Ensure 'row' and 'col' are integer sequence IDs
    matrix_coo = current_lp.mat.copy()
    matrix_coo["row"] = matrix_coo["row"].astype(int)
    matrix_coo["col"] = matrix_coo["col"].astype(int)

    if display_range:
        show_range_coo(matrix_coo, "\nUnscaled range     ")

    # Store accumulated scalers as Series, indexed by sequence ID
    # Initialize with 1.0 for all known rows/cols
    all_row_seq_ids = pd.Index(list(current_lp.seq_row.keys()), dtype=int, name="row")
    all_col_seq_ids = pd.Index(list(current_lp.seq_col.keys()), dtype=int, name="col")

    accumulated_sfs = {
        "row": pd.Series(1.0, index=all_row_seq_ids, dtype=float),
        "col": pd.Series(1.0, index=all_col_seq_ids, dtype=float),
    }

    for step_num in range(steps):
        # Calculate log10 of absolute values for the current matrix_coo
        log_matrix_coo = make_logdf_coo(matrix_coo)

        for s_entity in ["row", "col"]:  # "row" or "col" defines the entity to scale
            # Determine which entities (rows/cols) have coefficients outside 'bounds'
            # filter_df expects 'val' column to be log10(abs(coefficient))
            outlier_coeffs_df = filter_df(log_matrix_coo, bounds=bounds)

            if outlier_coeffs_df.empty:
                current_step_entity_sfs = pd.Series(dtype=float)  # No entities to scale
            else:
                # Get unique sequence IDs of entities that contain these outliers
                entities_to_scale_seq_ids = pd.Index(
                    outlier_coeffs_df[s_entity].unique(), dtype=int
                )

                if entities_to_scale_seq_ids.empty:
                    current_step_entity_sfs = pd.Series(dtype=float)
                else:
                    # Calculate min/max log for ALL coefficients in ALL entities (rows or cols)
                    # Group by the entity type ('row' or 'col' sequence ID column in log_matrix_coo)
                    all_entity_stats = log_matrix_coo.groupby(s_entity)["val"].agg(
                        min_log="min", max_log="max"
                    )

                    # Filter these stats to include only those entities marked for scaling
                    stats_for_scaling = all_entity_stats.loc[
                        all_entity_stats.index.isin(entities_to_scale_seq_ids)
                    ]

                    if stats_for_scaling.empty:
                        current_step_entity_sfs = pd.Series(dtype=float)
                    else:
                        # Calculate mid_log = mean of min_log and max_log
                        # Replace -np.inf (from log(0)) with a very small log value before mean, or handle carefully.
                        # If min_log is -inf and max_log is finite, mean is -inf.
                        # If both are -inf, mean is -inf.
                        # This means entities with only zero coeffs will get extreme scaling if not handled.
                        # However, make_logdf_coo sets log(0) to -np.inf.
                        # An entity (row/col) with only zero coefficients would have min_log = max_log = -np.inf.
                        # Then mid_log = -np.inf. 10**(-np.inf) or 10**(+np.inf) is 0 or inf. This is bad.

                        # Let's ensure mid_log is only calculated for finite min_log/max_log
                        finite_mask = np.isfinite(
                            stats_for_scaling["min_log"]
                        ) & np.isfinite(stats_for_scaling["max_log"])

                        mid_logs = pd.Series(
                            0.0, index=stats_for_scaling.index, dtype=float
                        )
                        if finite_mask.any():
                            mid_logs[finite_mask] = (
                                stats_for_scaling.loc[finite_mask, "min_log"]
                                + stats_for_scaling.loc[finite_mask, "max_log"]
                            ) / 2.0

                        # Convert to integer log exponent
                        mid_logs = np.int32(mid_logs)

                        # Exclude objective row/column from having their mid_log changed from 0
                        if s_entity == "row":
                            obj_row_seq = current_lp.gf_seq
                            if obj_row_seq in mid_logs.index:
                                mid_logs.loc[obj_row_seq] = 0
                        else:  # s_entity == "col"
                            constobj_seq_id = current_lp.col_name.get("constobj")
                            if (
                                constobj_seq_id is not None
                                and constobj_seq_id in mid_logs.index
                            ):
                                mid_logs.loc[constobj_seq_id] = 0

                        # Calculate scaling factors for the current step
                        if s_entity == "row":
                            current_step_entity_sfs = 10.0**mid_logs
                        else:  # s_entity == "col"
                            current_step_entity_sfs = 10.0 ** (-mid_logs)

            # Update accumulated scaling factors
            # Fill NaN with 1 for entities that were not scaled in this step_entity_sfs
            current_step_entity_sfs = current_step_entity_sfs.reindex(
                accumulated_sfs[s_entity].index
            ).fillna(1.0)
            accumulated_sfs[s_entity] = accumulated_sfs[s_entity].mul(
                current_step_entity_sfs
            )

            # Apply current_step_entity_sfs to matrix_coo for the next pass or step
            # Create a temporary DataFrame for merging
            sf_df_to_merge = current_step_entity_sfs.reset_index()
            sf_df_to_merge.columns = [s_entity, "sf_val"]  # s_entity column has seq_ids

            matrix_coo = matrix_coo.merge(sf_df_to_merge, on=s_entity, how="left")
            # Entities not in current_step_entity_sfs (e.g. not scaled this round) would get NaN sf_val
            # The reindex above should ensure sf_val is 1.0 for those.
            # Let's ensure sf_val is never NaN after merge if an entity was missing.
            matrix_coo["sf_val"].fillna(1.0, inplace=True)

            if s_entity == "row":
                matrix_coo["val"] /= matrix_coo["sf_val"]
            else:  # s_entity == "col"
                matrix_coo["val"] *= matrix_coo["sf_val"]
            matrix_coo.drop(columns=["sf_val"], inplace=True)

        if display_range:
            show_range_coo(matrix_coo, f"Scaled range step {step_num + 1}")

    # Generating prescaler arguments for GAMS
    gams_scaler_dict = {}
    for s_entity, sfs_series in accumulated_sfs.items():
        # Filter out scalers that are 1.0 (no effective scaling)
        sfs_to_apply = sfs_series[
            np.abs(sfs_series - 1.0) > 1e-9
        ]  # Check for not equal to 1

        for entity_seq_id, sf_val in sfs_to_apply.items():
            entity_name = ""
            if s_entity == "row":
                # lp.seq_row: key: row sequence, item: [row-name, lo_bnd, up_bond, type]
                entity_info = current_lp.seq_row.get(entity_seq_id)
                if entity_info:
                    entity_name = entity_info[0]
            else:  # s_entity == "col"
                # lp.seq_col: key: col-sequence, item: [col-name, lo_bnd, up_bond]
                entity_info = current_lp.seq_col.get(entity_seq_id)
                if entity_info:
                    entity_name = entity_info[0]

            if (
                not entity_name
            ):  # Should not happen if accumulated_sfs are indexed correctly
                print(
                    f"Warning: Could not find name for {s_entity} seq_id {entity_seq_id}"
                )
                continue

            # Apply GAMS specific naming convention
            k_ = ""
            if entity_name == "_obj":
                k_ = "_obj.scale"
            elif entity_name == "constobj":
                k_ = "constobj.scale"
            else:
                # This transformation is from the original make_scaler
                k_ = entity_name.replace("(", ".scale('")
                k_ = k_.replace(")", "')")
                k_ = k_.replace(",", "','")

            if k_:  # Ensure k_ was formed
                gams_scaler_dict[k_] = sf_val

    gams_scaler_dict["MESSAGE_LP.scaleopt"] = 1  # Activate scaling in GAMS

    scaler_df = pd.DataFrame(gams_scaler_dict, index=["val"]).transpose()
    scaler_df.index.name = "key"  # Match original output

    # --- Writing to GAMS file ---
    scaler_list_for_gams = []
    for k, v in gams_scaler_dict.items():
        scaler_list_for_gams.append(f"{k}={v};")
    scaler_args_gams_text = "\n".join(scaler_list_for_gams)

    # Determine output directory (similar to original)
    # Assuming 'scen' is an ixmp.Scenario object
    gams_file_name_parts = ["MsgScaler", scen.model, scen.scenario]
    gams_file_name = "_".join(s.replace(" ", "_") for s in gams_file_name_parts if s)

    # Path construction might need adjustment based on typical message_ix layout
    # For now, let's assume a 'model/scaler' subdirectory relative to where script might be run
    # Or use a more robust pathing. The original used os.getcwd() and '../..'
    # This is highly dependent on execution context.
    # For now, creating in current dir for simplicity if pathing fails.
    try:
        current_directory = (
            os.getcwd()
        )  # Or a more specific base like scen.model_path if available
        # This relative pathing is fragile.
        scaler_dir = os.path.join(current_directory, "model", "scaler")
        if not os.path.exists(scaler_dir):
            os.makedirs(scaler_dir, exist_ok=True)
        scaler_gms_path = os.path.join(scaler_dir, f"{gams_file_name}.gms")
    except Exception as e:
        print(
            f"Warning: Could not determine standard GAMS scaler path due to {e}. Writing to current directory."
        )
        scaler_gms_path = f"{gams_file_name}.gms"

    with open(scaler_gms_path, "w") as txtfile:
        txtfile.write(scaler_args_gams_text)
    print(f"Generated GAMS scaler file: {scaler_gms_path}")

    return scaler_df
